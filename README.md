# Proyecto 1 â€” Responsible IA (SecciÃ³n 10) âœ¨

**Universidad del Valle de Guatemala** â€” 29 de agosto de 2025
**Autor(es):**

* Andrea Ximena Ramirez Recinos (21874)
* Adrian Ricardo Flores Trujillo (21500)
* Daniel Armando Valdez Reyes (21240)

---

## ğŸ§­ Resumen

Este proyecto entrena y evalÃºa modelos de clasificaciÃ³n para el dataset **Adult (UCI)** con foco en **confiabilidad** y **mitigaciÃ³n de sesgos**:

* **Reâ€pesado simple:** `scale_pos_weight` + `sample_weight=fnlwgt` para manejar desbalance y respetar el diseÃ±o muestral.
* **Postâ€proceso barato:** **calibraciÃ³n por grupo** (isotÃ³nica/Platt) y **umbrales por grupo** (igualar *recall* o *precision*).
* **XGBoost con restricciones:** **monotonicidad** (no decrecer con `education-num`/`hours-per-week`) y **restricciÃ³n de interacciones** con atributos sensibles.
* **Explicabilidad estilo SHAP:** contribuciones direccionales con `pred_contribs=True` (TreeSHAP nativo de XGBoost).
* **Reporte reproducible en HTML:** mÃ©tricas, curvas ROC/PR, matriz de confusiÃ³n, importancias, grÃ¡ficos SHAP-like, **EDA** (distribuciones de objetivo, numÃ©ricas y categÃ³ricas) y tablas por grupo. Cada nuevo modelo **se concatena** como una nueva secciÃ³n en el mismo HTML.

---

## ğŸ—‚ï¸ Estructura (sugerida)

```
bias-mitigation/
â”œâ”€ EDA_Responsible.ipynb
â”œâ”€ report_modelos.html              
â”œâ”€ README.md
â””â”€ requirements.txt                 
```

> Si trabajÃ¡s en **Colab**, podÃ©s usar directamente el notebook y las celdas provistas.

---

## ğŸ“¦ Requisitos

**Python** â‰¥ 3.10

**Dependencias clave:**

* `numpy`, `pandas`, `matplotlib`
* `scikit-learn` â‰¥ **1.2**  (necesario para `OneHotEncoder(sparse_output=False)` y `set_output('pandas')`)
* `xgboost` â‰¥ **2.1** (para pasar `early_stopping_rounds` en el **constructor** del `XGBClassifier`)
* `ucimlrepo` (para cargar el dataset Adult automÃ¡ticamente)

Instalar:

```bash
pip install -r requirements.txt
```

O, en Colab:

```python
%pip -q install numpy pandas matplotlib scikit-learn>=1.2 xgboost>=2.1 ucimlrepo
```

---

## ğŸš€ Reproducir el proyecto (paso a paso)

1. **Cargar dataset Adult (UCI)**

```python
from ucimlrepo import fetch_ucirepo
adult = fetch_ucirepo(id=2)
X = adult.data.features
y = adult.data.targets  # binaria: <=50K / >50K
display(adult.variables)  # esquema de variables
```

2. **Imports base** (versiÃ³n limpia que usamos en el notebook)

```python
# Standard library
import base64, io, os, datetime as dt
from typing import Any, Dict, List, Optional, Tuple

# Third-party
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from IPython.display import display

# scikit-learn
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, average_precision_score, classification_report,
    confusion_matrix, f1_score, precision_recall_curve, precision_score,
    recall_score, roc_auc_score, roc_curve,
)

# XGBoost
import xgboost as xgb
from xgboost import XGBClassifier

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
```

3. **EDA** (opcional pero recomendado)
   â€“ Usar las funciones `make_y_hist_b64`, `make_numeric_grid_b64`, `make_categorical_grid_b64` y `export_eda_distributions_section` para **insertar EDA al HTML**.
   â€“ La **tabla de variables** (`adult.variables`) se incorpora con `export_dataset_variables_section`.

4. **Preprocesamiento + Pipeline**

   * `ColumnTransformer` con imputaciÃ³n, *standardization* numÃ©rica y `OneHotEncoder` **denso** (`sparse_output=False`).
   * `pre.set_output(transform="pandas")` para que XGBoost reciba nombres de columnas.

5. **Entrenamiento con reâ€pesado**

   * `sample_weight = X[fnlwgt]` y `scale_pos_weight` calculado con pesos.
   * **Early stopping**: pasar `early_stopping_rounds` en el **constructor** de `XGBClassifier` (no en `fit`).
   * Silenciar logs con `verbosity=0` y `clf__verbose=False`.

6. **Restricciones (solo XGBoost)**

   * **Monotonicidad**: `education-num` y `hours-per-week` con `+1`.
   * **Interacciones**: agrupar por **nombres** de columnas transformadas para limitar interacciones con sensibles (`sex`, `race`).

7. **ValidaciÃ³n, umbral y post-proceso**

   * SelecciÃ³n de **umbral por F1** en validaciÃ³n.
   * **CalibraciÃ³n por grupo** (isotÃ³nica/Platt) sobre scores de validaciÃ³n.
   * **Umbrales por grupo** para igualar *recall* (o *precision*) global.

8. **EvaluaciÃ³n final + Reporte HTML**

   * `evaluate_on_test_with_html(...)` genera/concatena secciones en `report_modelos.html`:
     mÃ©tricas, matriz de confusiÃ³n, **ROC**, **PR**, **importancias**, **SHAP-like**, **tablas por grupo**.
   * PodÃ©s correr mÃºltiples modelos (p.ej., XGB y luego LogReg) y se **agregan** secciones al mismo HTML.

---

## ğŸ§ª QuÃ© vas a ver en `report_modelos.html`

* **MÃ©tricas (Test):** Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC.
* **GrÃ¡ficas:**

  * Matriz de confusiÃ³n, curvas **ROC** y **Precisionâ€“Recall**.
  * **Importancias** (XGBoost) y **Contribuciones tipo SHAP** (magnitud, signo, niveles categÃ³ricos).
* **Post-proceso por grupo:** tabla con *precision/recall/F1* por subgrupo (p.ej., `sex`) y umbrales utilizados.
* **EDA:** distribuciones de la variable objetivo, numÃ©ricas y categÃ³ricas, con notas de confiabilidad.

> Cada corrida de `evaluate_on_test_with_html(...)` **anexa** una secciÃ³n con el nombre del modelo usado (`model_name`).

---

## ğŸ”— Recursos rÃ¡pidos

* **Notebook (.ipynb):** descarga directa desde GitHub Pages.
* **Preview HTML del notebook:** visualizaciÃ³n rÃ¡pida en el sitio.
* **Repositorio:** cÃ³digo fuente y notebooks.

---

## ğŸ§‘â€âš–ï¸ Nota sobre el uso de `fnlwgt`

`fnlwgt` es un **peso de diseÃ±o muestral**. En este proyecto lo usamos como `sample_weight` para **entrenamiento/validaciÃ³n**, **no** como feature predictiva, para no mezclar el proceso de muestreo con la seÃ±al del fenÃ³meno.


